{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77196f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from scipy.stats import uniform, randint\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from ollama import chat\n",
    "from ollama import ChatResponse\n",
    "import openai  \n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25cb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "news = pd.read_csv(r'F:\\Master\\thesis\\data collection\\tickers\\news.csv')\n",
    "bid = pd.read_csv(r'F:\\Master\\thesis\\data collection\\tickers\\2324bid.csv')\n",
    "ask = pd.read_csv(r'F:\\Master\\thesis\\data collection\\tickers\\2324ask.csv')\n",
    "event = pd.read_csv(r'F:\\Master\\thesis\\data collection\\RAG\\selected_fx_event.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16a97478",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['DateTime'] = pd.to_datetime(news['DateTime'])\n",
    "\n",
    "\n",
    "def round_to_nearest_hour(dt):\n",
    "    if dt.minute <= 30:\n",
    "        rounded_hour = dt.hour\n",
    "    else:\n",
    "        rounded_hour = (dt.hour + 1) % 24\n",
    "        if rounded_hour == 0:\n",
    "            dt = dt + pd.Timedelta(days=1)\n",
    "    return dt.replace(hour=rounded_hour, minute=0, second=0, microsecond=0)\n",
    "\n",
    "news['Timestamp'] = news['DateTime'].apply(round_to_nearest_hour)\n",
    "news['Timestamp'] = news['Timestamp'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55f3b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "event['DateTime'] = pd.to_datetime(event['DateTime'],utc=True)\n",
    "event['text'] = event.apply(\n",
    "    lambda row: f\"{row['Currency']} | {row['Impact']} | {row['Event']} | Actual: {row['Actual']} | Previous: {row['Previous']}\", \n",
    "    axis=1\n",
    ")\n",
    "event['DateTime'] = pd.to_datetime(event['DateTime'])\n",
    "\n",
    "def round_to_nearest_hour(dt):\n",
    "    if dt.minute <= 30:\n",
    "        rounded_hour = dt.hour\n",
    "    else:\n",
    "        rounded_hour = (dt.hour + 1) % 24\n",
    "        if rounded_hour == 0:\n",
    "            dt = dt + pd.Timedelta(days=1)\n",
    "    return dt.replace(hour=rounded_hour, minute=0, second=0, microsecond=0)\n",
    "\n",
    "event['Timestamp'] = event['DateTime'].apply(round_to_nearest_hour)\n",
    "event = event[['Timestamp', 'text']]\n",
    "\n",
    "event['Timestamp'] = event['Timestamp'].dt.tz_localize(None)\n",
    "event = event[['Timestamp', 'text']]\n",
    "news = news[['Timestamp', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e255531",
   "metadata": {},
   "outputs": [],
   "source": [
    "bid['Gmt time'] = pd.to_datetime(bid['Gmt time'], format='%d.%m.%Y %H:%M:%S.%f')\n",
    "ask['Gmt time'] = pd.to_datetime(ask['Gmt time'], format='%d.%m.%Y %H:%M:%S.%f')\n",
    "bid = bid[['Gmt time', 'Close']].rename(columns={'Close': 'Bid_Close'})\n",
    "ask = ask[['Gmt time', 'Close']].rename(columns={'Close': 'Ask_Close'})\n",
    "prices = pd.merge(bid, ask, on='Gmt time')\n",
    "prices['Mid_Close'] = (prices['Bid_Close'] + prices['Ask_Close']) / 2\n",
    "prices['Next_Mid_Close'] = prices['Mid_Close'].shift(-1)\n",
    "\n",
    "\n",
    "prices = prices.rename(columns={'Gmt time': 'Timestamp'})\n",
    "prices['Timestamp'] = prices['Timestamp'].dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02927234",
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da248d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    text = text.lower()\n",
    "\n",
    "    \n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "\n",
    "    \n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    \n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "\n",
    "    \n",
    "    cleaned_text = ' '.join(tokens)\n",
    "\n",
    "    \n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e48ae39",
   "metadata": {},
   "outputs": [],
   "source": [
    "news['cleaned_text'] = news['text'].apply(clean_text)\n",
    "news = news[['Timestamp', 'cleaned_text']]\n",
    "news = news.rename(columns={'cleaned_text': 'text'})\n",
    "\n",
    "news_prepared = news[['Timestamp', 'text']].copy()\n",
    "news_prepared['source'] = 'news'\n",
    "event_prepared = event[['Timestamp', 'text']].copy()\n",
    "event_prepared['source'] = 'event'\n",
    "\n",
    "text_data = pd.concat([news_prepared, event_prepared], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f875c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = text_data.merge(prices, on=\"Timestamp\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7489f466",
   "metadata": {},
   "source": [
    "# FinBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef3f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"yiyanghkust/finbert-tone\")\n",
    "finbert = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer, device=0, max_length=512)\n",
    "\n",
    "\n",
    "def get_sentiment_batch(merged_df, batch_size=32):\n",
    "    sentiments = []\n",
    "    \n",
    "    \n",
    "    for i in range(0, len(merged_df), batch_size):\n",
    "        \n",
    "        batch = merged_df.iloc[i:i+batch_size]['text'].tolist()\n",
    "        \n",
    "        \n",
    "        results = finbert(batch)\n",
    "        \n",
    "        \n",
    "        batch_sentiments = [result['label'] for result in results]\n",
    "        \n",
    "        \n",
    "        sentiments.extend(batch_sentiments)\n",
    "        \n",
    "    \n",
    "   \n",
    "    merged_df['sentiment'] = sentiments\n",
    "    return merged_df\n",
    "\n",
    "\n",
    "merged_df = get_sentiment_batch(merged_df)\n",
    "\n",
    "merged_df = merged_df.rename(columns={'sentiment': 'finbert_sent'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fef6872",
   "metadata": {},
   "source": [
    "## Codes for other LLMs Throw Ollama API (LLaMA2, LLaMA3, Gemma3, Gemma_fx, Deep2, Mistral)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f36d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_sentiment_forex_pairs = '''You are a financial analyst AI specialized in the Forex market, particularly EUR/USD currency movements.\n",
    "\n",
    "You will be given a short text containing either economic news or event summaries relevant to the Forex market.\n",
    "\n",
    "Analyze the text and determine the overall sentiment it conveys about the EUR/USD pair, based on how such content typically affects the market.\n",
    "\n",
    "Respond using only one of the following labels:\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "\n",
    "Strictly follow these rules:\n",
    "- Do not explain or justify your answer.\n",
    "- Do not use full sentences.\n",
    "- Do not include any punctuation or extra words.\n",
    "- Only output one of the three labels above exactly as written.\n",
    "- Never respond with anything outside of those three labels.'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75aa0485",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_base = \"http://localhost:11434/v1\"  \n",
    "openai.api_key = \"ollama3\"  \n",
    "\n",
    "def ask_llama3(input_content, system_prompt, deep_think=True, print_log=True):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"llama3.1:latest\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": input_content}\n",
    "        ]\n",
    "    )\n",
    "    response_text = response['choices'][0]['message']['content']\n",
    "    if print_log:\n",
    "        print(response_text)\n",
    "\n",
    "    \n",
    "    think_texts = re.findall(r'<think>(.*?)</think>', response_text, flags=re.DOTALL)\n",
    "    think_texts = \"\\n\\n\".join(think_texts).strip()\n",
    "    clean_response = re.sub(r'<think>.*?</think>', '', response_text, flags=re.DOTALL).strip()\n",
    "\n",
    "    return clean_response if not deep_think else (clean_response, think_texts)\n",
    "\n",
    "\n",
    "merged_df[['llama3.1_sent', 'llama3.1_THINK']] = merged_df['text'].apply(\n",
    "    lambda comment: ask_llama3(comment, system_prompt_sentiment_forex_pairs)\n",
    ").apply(pd.Series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9a6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = {\"Positive\": 1, \"Neutral\": 0, \"Negative\": -1}\n",
    "merged_df[\"finbert_sentiment\"] = merged_df[\"finbert_sentiment\"].map(sentiment_map)\n",
    "merged_df[\"llama3.1_sent\"] = merged_df[\"llama3.1_sent\"].map(sentiment_map)\n",
    "merged_df[\"gemma3.12_sent\"] = merged_df[\"gemma3.12_sent\"].map(sentiment_map)\n",
    "merged_df[\"deep2_sent\"] = merged_df[\"deep2_sent\"].map(sentiment_map)\n",
    "merged_df[\"llama2_sent\"] = merged_df[\"llama2_sent\"].map(sentiment_map)\n",
    "merged_df[\"gemma_fx_sent\"] = merged_df[\"gemma_fx_sent\"].map(sentiment_map)\n",
    "merged_df[\"mistral7b_sent\"] = merged_df[\"mistral7b_sent\"].map(sentiment_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb8a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.groupby(\"Timestamp\", as_index=False).agg({\n",
    "    \"Bid_Close\": \"first\",\n",
    "    \"Ask_Close\": \"first\",\n",
    "    \"Mid_Close\": \"first\",\n",
    "    \"Next_Mid_Close\": \"first\",\n",
    "    \"Simple_Return\": \"first\",\n",
    "    \"finbert_sent\": \"mean\",\n",
    "    \"llama3.1_sent\": \"mean\",\n",
    "    \"gemma3.12_sent\": \"mean\",\n",
    "    \"deep2_sent\": \"mean\",\n",
    "    \"llama2_sent\": \"mean\",\n",
    "    \"gemma_fx_sent\": \"mean\",\n",
    "    \"mistral7b_sent\" : \"mean\"\n",
    "})\n",
    "\n",
    "\n",
    "merged_df = merged_df.round(4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbd39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_step = 48\n",
    "test_size = 0.15\n",
    "val_size = 0.15\n",
    "\n",
    "def create_sequences(data, time_step=48):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        X.append(data[i:i + time_step, :-1])\n",
    "        y.append(data[i + time_step, -1])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def create_lagged_features(X, y, n_lags=48):\n",
    "    X_lagged, y_lagged = [], []\n",
    "    for i in range(n_lags, len(X)):\n",
    "        X_lagged.append(X[i - n_lags:i].flatten())\n",
    "        y_lagged.append(y[i])\n",
    "    return np.array(X_lagged), np.array(y_lagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3771e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These columns are for baseline experiment and for sentiment analysis we will add sentiment column of each LLM\n",
    "df = merged_df[['Bid_Open','Bid_High','Bid_Low','Bid_Close', 'Bid_Volume',\n",
    "          'Ask_Open','Ask_High','Ask_Low','Ask_Close','Ask_Volume','target']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cd512c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_len = len(df)\n",
    "train_end = int(data_len * 0.7)\n",
    "val_end = int(data_len * 0.85)\n",
    "\n",
    "train_df = df.iloc[:train_end]\n",
    "val_df = df.iloc[train_end:val_end]\n",
    "test_df = df.iloc[val_end:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c968a7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "train_scaled = scaler.fit_transform(train_df)\n",
    "val_scaled = scaler.transform(val_df)\n",
    "test_scaled = scaler.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80943dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scaled = np.vstack([train_scaled, val_scaled, test_scaled])\n",
    "X_all, y_all = create_sequences(all_scaled, time_step)\n",
    "\n",
    "split1 = train_end - time_step\n",
    "split2 = val_end - time_step\n",
    "\n",
    "X_train, y_train = X_all[:split1], y_all[:split1]\n",
    "X_val, y_val = X_all[split1:split2], y_all[split1:split2]\n",
    "X_test, y_test = X_all[split2:], y_all[split2:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8df784",
   "metadata": {},
   "source": [
    "# --- Build and Train GRU --- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8718d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_gru_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(GRU(256, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(GRU(128, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='mse')\n",
    "    return model\n",
    "\n",
    "early_stop = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "gru_model = build_gru_model((X_train.shape[1], X_train.shape[2]))\n",
    "gru_model.fit(X_train, y_train,\n",
    "              validation_data=(X_val, y_val),\n",
    "              epochs=50,\n",
    "              batch_size=128,\n",
    "              callbacks=[early_stop],\n",
    "              verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1d3052",
   "metadata": {},
   "source": [
    "# --- Build and Train LSTM --- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749397a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(256, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(128, return_sequences=False))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='mse')\n",
    "    return model\n",
    "\n",
    "lstm_model = build_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "lstm_model.fit(X_train, y_train,\n",
    "               validation_data=(X_val, y_val),\n",
    "               epochs=50,\n",
    "               batch_size=128,\n",
    "               callbacks=[early_stop],\n",
    "               verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213b7b7",
   "metadata": {},
   "source": [
    "# --- Evaluate GRU and LSTM --- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d952f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X, y, name):\n",
    "    y_pred = model.predict(X)\n",
    "    y_true_scaled = y.reshape(-1, 1)\n",
    "    y_pred_scaled = y_pred.reshape(-1, 1)\n",
    "\n",
    "    pad_true = np.zeros((len(y_true_scaled), all_scaled.shape[1]))\n",
    "    pad_true[:, -1] = y_true_scaled.flatten()\n",
    "    pad_pred = np.zeros_like(pad_true)\n",
    "    pad_pred[:, -1] = y_pred_scaled.flatten()\n",
    "\n",
    "    y_true_real = scaler.inverse_transform(pad_true)[:, -1]\n",
    "    y_pred_real = scaler.inverse_transform(pad_pred)[:, -1]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true_real, y_pred_real))\n",
    "    mae = mean_absolute_error(y_true_real, y_pred_real)\n",
    "    mape = np.mean(np.abs((y_true_real - y_pred_real) / y_true_real)) * 100\n",
    "    r2 = r2_score(y_true_real, y_pred_real)\n",
    "\n",
    "    print(f\"{name} RMSE: {rmse:.5f}, MAE: {mae:.5f}, MAPE: {mape:.2f}%, RÂ²: {r2:.4f}\")\n",
    "\n",
    "\n",
    "evaluate_model(gru_model, X_test, y_test, \"GRU\")\n",
    "evaluate_model(lstm_model, X_test, y_test, \"LSTM\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "992bcf4e",
   "metadata": {},
   "source": [
    "# --- XGBoost --- #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bc7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_lag, y_all_lag = create_lagged_features(X_all, y_all, n_lags=60)\n",
    "X_train_lag = X_all_lag[:split1 - 60]\n",
    "y_train_lag = y_all_lag[:split1 - 60]\n",
    "X_val_lag = X_all_lag[split1 - 60:split2 - 60]\n",
    "y_val_lag = y_all_lag[split1 - 60:split2 - 60]\n",
    "X_test_lag = X_all_lag[split2 - 60:]\n",
    "y_test_lag = y_all_lag[split2 - 60:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa2cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='hist',        \n",
    "    device='cuda',           \n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "xgb_model.fit(\n",
    "    X_train_lag, y_train_lag,\n",
    "    eval_set=[(X_val_lag, y_val_lag)],\n",
    "    verbose=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71e228e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_xgb = xgb_model.predict(X_test_lag)\n",
    "\n",
    "\n",
    "target_scaler = MinMaxScaler()\n",
    "target_scaler.fit(train_df[['target']])\n",
    "y_test_real = target_scaler.inverse_transform(y_test_lag.reshape(-1, 1)).flatten()\n",
    "y_pred_real = target_scaler.inverse_transform(y_pred_xgb.reshape(-1, 1)).flatten()\n",
    "\n",
    "\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_real, y_pred_real))\n",
    "mae_xgb = mean_absolute_error(y_test_real, y_pred_real)\n",
    "mape_xgb = np.mean(np.abs((y_test_real - y_pred_real) / y_test_real)) * 100\n",
    "r2_xgb = r2_score(y_test_real, y_pred_real)\n",
    "\n",
    "\n",
    "print(f\"XGBoost RMSE: {rmse_xgb:.5f}, MAE: {mae_xgb:.5f}, MAPE: {mape_xgb:.2f}%, R2:{r2_xgb:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
